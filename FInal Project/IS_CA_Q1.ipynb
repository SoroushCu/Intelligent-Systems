{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy torch torchvision pytorch-ignite tensorboardX tensorboard opendatasets efficientnet-pytorch\n",
        "!pip3 install --upgrade alexnet_pytorch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms as T\n",
        "from torchvision import models, datasets\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss, Precision, Recall\n",
        "from ignite.handlers import LRScheduler, ModelCheckpoint, global_step_from_engine\n",
        "from ignite.contrib.handlers import ProgressBar, TensorboardLogger\n",
        "import ignite.contrib.engines.common as common\n",
        "import opendatasets as od\n",
        "import os\n",
        "from random import randint\n",
        "import urllib\n",
        "import zipfile\n",
        "from alexnet_pytorch import AlexNet"
      ],
      "metadata": {
        "id": "WX8QFD_4aZii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "eMnH8l8Af10G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -qq 'tiny-imagenet-200.zip'\n",
        "DATA_DIR = 'tiny-imagenet-200' \n",
        "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
        "VALID_DIR = os.path.join(DATA_DIR, 'val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx3pPx5kgMza",
        "outputId": "b3971935-fccf-4c7b-b585-c8a1f8387836"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-05 17:47:10--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip.1’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  15.7MB/s    in 19s     \n",
            "\n",
            "2023-02-05 17:47:29 (12.8 MB/s) - ‘tiny-imagenet-200.zip.1’ saved [248100043/248100043]\n",
            "\n",
            "replace tiny-imagenet-200/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "    \n",
        "def show_batch(dataloader):\n",
        "    dataiter = iter(dataloader)\n",
        "    images, labels = dataiter.next()    \n",
        "    imshow(make_grid(images))\n",
        "    \n",
        "def show_image(dataloader):\n",
        "    dataiter = iter(dataloader)\n",
        "    images, labels = dataiter.next()\n",
        "    random_num = randint(0, len(images)-1)\n",
        "    imshow(images[random_num])\n",
        "    label = labels[random_num]\n",
        "    print(f'Label: {label}, Shape: {images[random_num].shape}')"
      ],
      "metadata": {
        "id": "aef-HIE7gXpS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataloader(data, name, transform):\n",
        "    if data is None: \n",
        "        return None\n",
        "    if transform is None:\n",
        "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
        "    else:\n",
        "        dataset = datasets.ImageFolder(data, transform=transform)\n",
        "    # idx = (dataset.targets==1) | (dataset.targets==2) | (dataset.targets==3) | (dataset.targets==4)\n",
        "    # dataset.targets = dataset.targets[idx]\n",
        "    # dataset.data = dataset.data[idx]/\n",
        "    if use_cuda:\n",
        "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
        "    else:\n",
        "        kwargs = {}\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                        shuffle=(name==\"train\"), \n",
        "                        **kwargs)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "XfcJ1hSAgc22"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_img_dir = os.path.join(VALID_DIR, 'images')\n",
        "fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\n",
        "data = fp.readlines()\n",
        "val_img_dict = {}\n",
        "for line in data:\n",
        "    words = line.split('\\t')\n",
        "    val_img_dict[words[0]] = words[1]\n",
        "fp.close()\n",
        "{k: val_img_dict[k] for k in list(val_img_dict)[:10]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sjLr4ZKgudW",
        "outputId": "4c89b62d-4962-4ba3-bddb-7a5460b2ec17"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_0.JPEG': 'n03444034',\n",
              " 'val_1.JPEG': 'n04067472',\n",
              " 'val_2.JPEG': 'n04070727',\n",
              " 'val_3.JPEG': 'n02808440',\n",
              " 'val_4.JPEG': 'n02808440',\n",
              " 'val_5.JPEG': 'n04399382',\n",
              " 'val_6.JPEG': 'n04179913',\n",
              " 'val_7.JPEG': 'n02823428',\n",
              " 'val_8.JPEG': 'n04146614',\n",
              " 'val_9.JPEG': 'n02226429'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img, folder in val_img_dict.items():\n",
        "    newpath = (os.path.join(val_img_dir, folder))\n",
        "    if not os.path.exists(newpath):\n",
        "        os.makedirs(newpath)\n",
        "    if os.path.exists(os.path.join(val_img_dir, img)):\n",
        "        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))"
      ],
      "metadata": {
        "id": "O_bOyOV5g8Jc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_transform_pretrain = T.Compose([\n",
        "                T.Resize(256),\n",
        "                T.CenterCrop(224),\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                            std=[0.229, 0.224, 0.225])])"
      ],
      "metadata": {
        "id": "0dH4O6FjhBk7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataloader = generate_dataloader(TRAIN_DIR, \"train\",\n",
        "                                  transform=preprocess_transform_pretrain)\n",
        "test_dataloader = generate_dataloader(val_img_dir, \"val\",\n",
        "                                 transform=preprocess_transform_pretrain)"
      ],
      "metadata": {
        "id": "3uyISgavhEtl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_alexnet = models.alexnet(pretrained=True)"
      ],
      "metadata": {
        "id": "EZ2KZphnhOfs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model_alexnet.named_parameters() :\n",
        "    param.requires_grad = False\n",
        "    if name.startswith('classifier') : \n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "UotPlmqJJpc7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetConv2(nn.Module):\n",
        "    def init(self):\n",
        "        super(AlexNetConv2, self).init()\n",
        "        self.features = nn.Sequential(*list(model_alexnet.features.children())[:-7])\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x\n",
        "\n",
        "class AlexNetConv5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNetConv5, self).__init__()\n",
        "        self.featuresdfd = nn.Sequential(*list(model_alexnet.features.children())[:-4])\n",
        "    def forward(self, x):\n",
        "        x = self.featuresdfd(x)\n",
        "        return x\n",
        "\n",
        "class AlexNetFc8(nn.Module):\n",
        "    def init(self):\n",
        "        super(AlexNetFc8, self).init()\n",
        "        self.features = nn.Sequential(*list(model_alexnet.features.children())[:-1])\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jMDcpRSwh6KP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel_conv5(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(MyModel_conv5, self).__init__()\n",
        "    self.pre_trained = model\n",
        "    self.seq = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, 3, 1),\n",
        "            nn.Conv2d(256, 256, 3, 1),\n",
        "            nn.Conv2d(256, 256, 3, 1),\n",
        "            nn.ConvTranspose2d(256, 256, 5, 2),\n",
        "            nn.ConvTranspose2d(256, 128, 5, 2),\n",
        "            nn.ConvTranspose2d(128, 64, 5, 2),\n",
        "            nn.ConvTranspose2d(64, 32, 5, 2),\n",
        "            nn.ConvTranspose2d(32, 3, 5, 2), \n",
        "            nn.MaxPool2d((94, 94), stride=(1, 1)), \n",
        "            )\n",
        "          \n",
        "  def forward(self, x):\n",
        "    x = self.pre_trained(x)\n",
        "    x = self.seq(x)\n",
        "    return x\n",
        "\n",
        "pre_trained = AlexNetConv5()\n",
        "model = MyModel_conv5(pre_trained)"
      ],
      "metadata": {
        "id": "8vk7T13y9JXD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "metadata": {
        "id": "XmrxgITzDHHk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001  \n",
        "num_epochs = 3 \n",
        "log_interval = 300\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "eYNXiRA_je6t"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    loss_values = []\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, X)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        loss_values.append(running_loss)\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    plt.plot(np.array(loss_values), 'r')"
      ],
      "metadata": {
        "id": "6ojxeoavHltY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    loss_values = []\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, X).item()\n",
        "            correct += (pred.argmax(1) == X).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "i3WNlqrfjxcK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_func, optimizer)\n",
        "    test(test_dataloader, model, loss_func)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "4Kf9TlbrKC8m",
        "outputId": "81c689a3-b586-4337-90fc-29777a5180a4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.257729  [    0/100000]\n",
            "loss: 6.333055  [ 6400/100000]\n",
            "loss: 1.987759  [12800/100000]\n",
            "loss: 1.413734  [19200/100000]\n",
            "loss: 1.384520  [25600/100000]\n",
            "loss: 1.442773  [32000/100000]\n",
            "loss: 1.484911  [38400/100000]\n",
            "loss: 1.264908  [44800/100000]\n",
            "loss: 1.304636  [51200/100000]\n",
            "loss: 1.251667  [57600/100000]\n",
            "loss: 1.091855  [64000/100000]\n",
            "loss: 1.137109  [70400/100000]\n",
            "loss: 1.184907  [76800/100000]\n",
            "loss: 1.285305  [83200/100000]\n",
            "loss: 1.299947  [89600/100000]\n",
            "loss: 1.248004  [96000/100000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8a9cbef9a221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-3c1d0e47c27d>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (3) at non-singleton dimension 1"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZBiO4qKKc7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "foXiTQZVQzpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}